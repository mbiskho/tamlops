{
    "title": "Chaos Offline ML Inference Server Experiment",
    "description": "This experiment is to test the load testing performance & find the errors when ML Inference is offline (offline)",
    "tags": [],
    "steady-state-hypothesis": {
        "title": "Make sure that load testing has been done & able to prompt every types to server",
        "probes": [
            {
                "name": "Normal load success rate testing log must exists",
                "type": "probe",
                "tolerance": true,
                "provider": {
                    "type": "python",
                    "module": "os.path",
                    "func": "exists",
                    "arguments": {
                        "path": "log/running/load-successrate-testing-steadystate.log"
                    }
                }
            },
            {
                "name": "Normal load testing log must exists",
                "type": "probe",
                "tolerance": true,
                "provider": {
                    "type": "python",
                    "module": "os.path",
                    "func": "exists",
                    "arguments": {
                        "path": "log/running/load-testing-steadystate.log"
                    }
                }
            },
            {
                "type": "probe",
                "name": "We can request text",
                "tolerance": 200,
                "provider": {
                    "type": "http",
                    "timeout": 20,
                    "url": "http://35.208.32.246:8000/inference",
                    "method": "POST",
                    "headers": {
                        "Content-Type": "application/json"
                    },
                    "arguments": {
                        "type": "text",
                        "text": "What german name of apple"
                    }
                }
            },
            {
                "type": "probe",
                "name": "We can request image",
                "tolerance": 200,
                "provider": {
                    "type": "http",
                    "timeout": 60,
                    "url": "http://35.208.32.246:8000/inference",
                    "method": "POST",
                    "headers": {
                        "Content-Type": "application/json"
                    },
                    "arguments": {
                        "type": "image",
                        "text": "Make me an image of a cat with hat"
                    }
                }
            }
        ]
    },
    "method": [
        {
            "type": "action",
            "name": "Sleep to give time for turning off the Inference pod",
            "background": false,
            "provider": {
                "type": "process",
                "path": "sleep",
                "arguments": "60"
            }
        },
        {
            "type": "action",
            "name": "Run load success rate testing",
            "background": false,
            "provider": {
                "type": "process",
                "path": "K6_PROMETHEUS_RW_TREND_AS_NATIVE_HISTOGRAM=true",
                "arguments": "k6 run -o experimental-prometheus-rw load-successrate-testing.js"
            },
            "controls": [
                {
                    "name": "Change CHAOS_TYPE env value to off-interface",
                    "scope": "before",
                    "provider": {
                        "type": "process",
                        "path": "export",
                        "arguments": "CHAOS_TYPE=off-interface"
                    }
                }
            ]
        },
        {
            "type": "action",
            "name": "Run load testing",
            "background": false,
            "provider": {
                "type": "process",
                "path": "K6_PROMETHEUS_RW_TREND_AS_NATIVE_HISTOGRAM=true",
                "arguments": "k6 run -o experimental-prometheus-rw load-testing.js"
            },
            "controls": [
                {
                    "name": "Change CHAOS_TYPE env value to off-inference",
                    "scope": "before",
                    "provider": {
                        "type": "process",
                        "path": "export",
                        "arguments": "CHAOS_TYPE=off-inference"
                    }
                }
            ]
        },
        {
            "type": "action",
            "name": "Run load testing",
            "background": false,
            "provider": {
                "type": "process",
                "path": "K6_PROMETHEUS_RW_TREND_AS_NATIVE_HISTOGRAM=true",
                "arguments": "k6 run -o experimental-prometheus-rw load-testing.js"
            },
            "controls": [
                {
                    "name": "Change CHAOS_TYPE env value to off-inference",
                    "scope": "before",
                    "provider": {
                        "type": "process",
                        "path": "export",
                        "arguments": "CHAOS_TYPE=off-inference"
                    }
                }
            ]
        },
        {
            "type": "action",
            "name": "Sleep to give time for turning on the Inference pods",
            "background": false,
            "provider": {
                "type": "process",
                "path": "sleep",
                "arguments": "60"
            }
        }
    ],
    "rollbacks": []
}